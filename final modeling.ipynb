{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports. Nothing to see here.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import string\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from gensim.models import Word2Vec\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist, word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, plot_confusion_matrix, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "source": [
    "# EDA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Here we check out the data. We explore, look for inconsistencies, and use what we see to define our modeling plan. We start with general data cleaning practice, checking for duplicates and missing values. As we get deeper into the data, our techniques get more specific to the data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iPhone   \n",
       "1              iPad or iPhone App   \n",
       "2                            iPad   \n",
       "3              iPad or iPhone App   \n",
       "4                          Google   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                   Negative emotion  \n",
       "1                                   Positive emotion  \n",
       "2                                   Positive emotion  \n",
       "3                                   Negative emotion  \n",
       "4                                   Positive emotion  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet_text</th>\n      <th>emotion_in_tweet_is_directed_at</th>\n      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n      <td>iPhone</td>\n      <td>Negative emotion</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n      <td>iPad or iPhone App</td>\n      <td>Positive emotion</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n      <td>iPad</td>\n      <td>Positive emotion</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>@sxsw I hope this year's festival isn't as cra...</td>\n      <td>iPad or iPhone App</td>\n      <td>Negative emotion</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n      <td>Google</td>\n      <td>Positive emotion</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# Import and inspect data\n",
    "\n",
    "data = pd.read_csv('data.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'.@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.'"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# Read content. These are a dataset of Tweets from SXSW in Austin from 2011.\n",
    "\n",
    "data.iloc[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in the primary column and remove the one we find.\n",
    "\n",
    "print(data['tweet_text'].isna().sum())\n",
    "\n",
    "data = data[~data['tweet_text'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "# Number of duplicate rows dropped: 22\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicated rows and preserve unique entries.\n",
    "\n",
    "a = len(data)\n",
    "data = data.drop_duplicates()\n",
    "b = len(data)\n",
    "print('# Number of duplicate rows dropped: {}'.format(a-b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "iPad                               945\nApple                              659\niPad or iPhone App                 469\nGoogle                             428\niPhone                             296\nOther Google product or service    293\nAndroid App                         80\nAndroid                             77\nOther Apple product or service      35\nName: emotion_in_tweet_is_directed_at, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Explore and simplify. We're defining our project goals more closely here.\n",
    "\n",
    "print(data['emotion_in_tweet_is_directed_at'].value_counts())\n",
    "\n",
    "company = {'iPad': 'Apple',\n",
    "            'Apple': 'Apple',\n",
    "            'iPad or iPhone App': 'Apple',\n",
    "            'Google': 'Google',\n",
    "            'iPhone': 'Apple',\n",
    "            'Other Google product or service': 'Google',\n",
    "            'Android App': 'Google',\n",
    "            'Android': 'Google',\n",
    "            'Other Apple product or service': 'Apple'}\n",
    "            \n",
    "data['emotion_in_tweet_is_directed_at'] = data['emotion_in_tweet_is_directed_at'].map(company)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplify column names for convenience.\n",
    "\n",
    "data.rename(columns={'tweet_text': 'text', 'emotion_in_tweet_is_directed_at': 'brand', 'is_there_an_emotion_directed_at_a_brand_or_product': 'feelings'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original dataset values:\n",
      " No emotion toward brand or product    5375\n",
      "Positive emotion                      2970\n",
      "Negative emotion                       569\n",
      "I can't tell                           156\n",
      "Name: feelings, dtype: int64 \n",
      "\n",
      "Encoded and chosen dataset values:\n",
      " 1    2970\n",
      "0     569\n",
      "Name: feelings, dtype: int64 \n",
      "\n",
      "Total entries: 3539\n"
     ]
    }
   ],
   "source": [
    "# Here we inspect and encode the labels for our target column. We also define the boundaries of our task, narrowing our dataset to entries with clear positive or negative expressions.\n",
    "\n",
    "print('Original dataset values:\\n', data['feelings'].value_counts(), '\\n')\n",
    "\n",
    "feels = {'Negative emotion': 0,\n",
    "        'Positive emotion': 1,\n",
    "        'No emotion toward brand or product': 2,\n",
    "        \"I can't tell\": 3}\n",
    "\n",
    "data['feelings'] = data['feelings'].map(feels)\n",
    "\n",
    "data = data[data['feelings'] <= 1]\n",
    "\n",
    "print('Encoded and chosen dataset values:\\n', data['feelings'].value_counts(), '\\n')\n",
    "\n",
    "print('Total entries:', len(data))"
   ]
  },
  {
   "source": [
    "# Now the NLP Begins"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Our data is clean and ready to be processed. Now we process. We start by creating tools, then use the tools on the Tweet text to reshape the data into a manageable and meaningful form."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tools to process the Tweets. We use Regex to narrow our data to words, and stopwords to return only the words that are significant.\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "stops = stopwords.words('english')\n",
    "stops += list(string.punctuation)\n",
    "stops.extend(['sxsw', 'sxswi', 'quot', 'mention', 'link', 'rt', 'amp', 'http', 'sxswrt', 'google', 'googles', 'app', 'apps', 'android', 'austin', 'quotgoogle', 'new', 'today', 'one', 'apple', 'ipad', 'iphone', 'ipad2', 'apples', 'quotapple','store'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                   text   brand  feelings  \\\n",
       "0     .@wesley83 I have a 3G iPhone. After 3 hrs twe...   Apple         0   \n",
       "1     @jessedee Know about @fludapp ? Awesome iPad/i...   Apple         1   \n",
       "2     @swonderlin Can not wait for #iPad 2 also. The...   Apple         1   \n",
       "3     @sxsw I hope this year's festival isn't as cra...   Apple         0   \n",
       "4     @sxtxstate great stuff on Fri #SXSW: Marissa M...  Google         1   \n",
       "...                                                 ...     ...       ...   \n",
       "9077  @mention your PR guy just convinced me to swit...   Apple         1   \n",
       "9079  &quot;papyrus...sort of like the ipad&quot; - ...   Apple         1   \n",
       "9080  Diller says Google TV &quot;might be run over ...  Google         0   \n",
       "9085  I've always used Camera+ for my iPhone b/c it ...   Apple         1   \n",
       "9088                      Ipad everywhere. #SXSW {link}   Apple         1   \n",
       "\n",
       "                                                 tokens  \n",
       "0     [wesley83, 3g, 3, hrs, tweeting, rise_austin, ...  \n",
       "1     [jessedee, know, fludapp, awesome, likely, app...  \n",
       "2                     [swonderlin, wait, 2, also, sale]  \n",
       "3                  [hope, year, festival, crashy, year]  \n",
       "4     [sxtxstate, great, stuff, fri, marissa, mayer,...  \n",
       "...                                                 ...  \n",
       "9077  [pr, guy, convinced, switch, back, great, cove...  \n",
       "9079          [papyrus, sort, like, nice, lol, lavelle]  \n",
       "9080  [diller, says, tv, might, run, playstation, xb...  \n",
       "9085  [always, used, camera, b, c, image, stabilizer...  \n",
       "9088                                       [everywhere]  \n",
       "\n",
       "[3539 rows x 4 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>brand</th>\n      <th>feelings</th>\n      <th>tokens</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n      <td>Apple</td>\n      <td>0</td>\n      <td>[wesley83, 3g, 3, hrs, tweeting, rise_austin, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n      <td>Apple</td>\n      <td>1</td>\n      <td>[jessedee, know, fludapp, awesome, likely, app...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n      <td>Apple</td>\n      <td>1</td>\n      <td>[swonderlin, wait, 2, also, sale]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>@sxsw I hope this year's festival isn't as cra...</td>\n      <td>Apple</td>\n      <td>0</td>\n      <td>[hope, year, festival, crashy, year]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n      <td>Google</td>\n      <td>1</td>\n      <td>[sxtxstate, great, stuff, fri, marissa, mayer,...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9077</th>\n      <td>@mention your PR guy just convinced me to swit...</td>\n      <td>Apple</td>\n      <td>1</td>\n      <td>[pr, guy, convinced, switch, back, great, cove...</td>\n    </tr>\n    <tr>\n      <th>9079</th>\n      <td>&amp;quot;papyrus...sort of like the ipad&amp;quot; - ...</td>\n      <td>Apple</td>\n      <td>1</td>\n      <td>[papyrus, sort, like, nice, lol, lavelle]</td>\n    </tr>\n    <tr>\n      <th>9080</th>\n      <td>Diller says Google TV &amp;quot;might be run over ...</td>\n      <td>Google</td>\n      <td>0</td>\n      <td>[diller, says, tv, might, run, playstation, xb...</td>\n    </tr>\n    <tr>\n      <th>9085</th>\n      <td>I've always used Camera+ for my iPhone b/c it ...</td>\n      <td>Apple</td>\n      <td>1</td>\n      <td>[always, used, camera, b, c, image, stabilizer...</td>\n    </tr>\n    <tr>\n      <th>9088</th>\n      <td>Ipad everywhere. #SXSW {link}</td>\n      <td>Apple</td>\n      <td>1</td>\n      <td>[everywhere]</td>\n    </tr>\n  </tbody>\n</table>\n<p>3539 rows × 4 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# Use our new tools!\n",
    "\n",
    "data['tokens'] = data['text'].apply(tokenizer.tokenize)\n",
    "\n",
    "# This is inefficient and can be worked to be better. First pass eliminates stopwords that are already lowercase, and turns the rest of the words lowercase. Second pass removes the remaining stopwords. It also leaves our data in list form, which can be an issue later. Our lemmatizing function is built to work with a list. We fix the issue before we Count Vectorize or TD-IDF in the main function below.\n",
    "data['tokens'] = data['tokens'].apply(lambda x: [word.lower() for word in x if word not in stops])\n",
    "data['tokens'] = data['tokens'].apply(lambda x: [word.lower() for word in x if word not in stops])\n",
    "\n",
    "data"
   ]
  },
  {
   "source": [
    "# Creating Functions for User Interface"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### It is important that we create functions for each of the modeling steps. We will be selecting which ones to use individually later and they all need to be defined before we know which ones will be used. It is extra important that they all work well individually and also do not interfere with each other."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Vectorize or TF-IDF. Our first and most vital choice.\n",
    "def CV(X_train, X_test):\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    X_train_counts = count_vectorizer.fit_transform(X_train)\n",
    "    X_test_counts = count_vectorizer.transform(X_test)\n",
    "    return X_train_counts, X_test_counts\n",
    "\n",
    "def tf_idf(X_train, X_test):\n",
    "    tfidf = TfidfVectorizer()\n",
    "    X_train_counts = tfidf.fit_transform(X_train)\n",
    "    X_test_counts = tfidf.transform(X_test)\n",
    "    return X_test_counts, X_train_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizing will or won't happen. Two functions, one nested inside the other.\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(w) for w in text]\n",
    "\n",
    "def lemmatize():\n",
    "    data['lemm'] = data['tokens'].apply(lemmatize_text)\n",
    "    data['lemm'] = data['lemm'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE either will or will not run.\n",
    "def smote(X_train_counts, y_train):\n",
    "    smote = SMOTE()\n",
    "    X_train_counts, y_train = smote.fit_sample(X_train_counts, y_train)\n",
    "    return X_train_counts, y_train\n",
    "\n",
    "# Train Test Split. The col variable is important and will be different depending on whether we lemmatize.\n",
    "def TTS(col):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[col], data['feelings'])\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "def logreg(X_train_counts, y_train, X_test_counts):\n",
    "    clf = LogisticRegression(C=30.0, class_weight='balanced', solver='newton-cg', \n",
    "                         multi_class='multinomial', n_jobs=-1, random_state=40)\n",
    "    clf.fit(X_train_counts, y_train)\n",
    "    y_predicted_counts = clf.predict(X_test_counts)\n",
    "    return y_predicted_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "def rf(X_train_counts, y_train, X_test_counts):\n",
    "    rf = RandomForestClassifier()\n",
    "    rf.fit(X_train_counts, y_train)\n",
    "    y_predicted_counts = rf.predict(X_test_counts)\n",
    "    return y_predicted_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinomial Naive Bayes\n",
    "def multiNB(X_train_counts, y_train, X_test_counts):\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train_counts, y_train)\n",
    "    y_predicted_counts = nb.predict(X_test_counts)\n",
    "    return y_predicted_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics that will work with any model selected, along with a confusion matrix.\n",
    "def classify(y_test, y_predicted_counts):\n",
    "    print('\\n\\nClassification Report - TEST')\n",
    "    print('--------------------------------------------------------------------------')\n",
    "    print(classification_report(y_test, y_predicted_counts))\n",
    "    print('--------------------------------------------------------------------------')\n",
    "    print('Confusion Matrix - TEST')\n",
    "    print('--------------------------------------------------------------------------')\n",
    "    print(pd.crosstab(y_test, y_predicted_counts, rownames=['True'], colnames=['Predicted'], margins=True))\n",
    "    print('--------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Our Modeling System!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### We made the choice to create an interactive modeling system. The user will be asked to supply inputs and choose his own path to model the data. First, we choose to Count Vectorize or TF-IDF, whether or not to Lemmatize or to SMOTE. There are 8 possible combinations of choices, each accounted for. We've even processed every combination beforehand to have recommended modeling depending on the user's choices. Modeling is fun again!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_models(data):\n",
    "\n",
    "    # Prepare some variables.\n",
    "    col = None\n",
    "    model = None\n",
    "    lemm = None\n",
    "    smt = None\n",
    "    reco = None\n",
    "    picks = []\n",
    "    choices = [logreg, rf, multiNB]\n",
    "    t = None\n",
    "\n",
    "    # User choices will influence recommended model selection.\n",
    "    log = [[1, 2, 2], [2,2,2]]\n",
    "    forest = [[1,1,2],[2,1,2]]\n",
    "    NB = [[1,2,1],[2,2,1],[1,1,1,],[2,1,1]]\n",
    "\n",
    "    # Canned response for every response that isn't '1' or '2.'\n",
    "    jerk = \"Don't waste my time. Try again wiseguy.\"\n",
    "\n",
    "    # Here we ask three binary questions, leading to 8 possible combinations. The answers affect what functions are run, and also are saved in a list and used to compare for model selection.\n",
    "    print('How would you like to analyze the data?\\nType \"1\" to Count Vectorize or \"2\" to implement TF-IDF.\\n')\n",
    "    model = input()\n",
    "    picks.append(int(model))\n",
    "    if model not in ['1', '2']:\n",
    "        return print(jerk)\n",
    "\n",
    "    print('Would you like to lemmatize?\\nType \"1\" for Yes or \"2\" for No.\\n')\n",
    "    lemm  = input()\n",
    "    picks.append(int(lemm))\n",
    "    if lemm not in ['1', '2']:\n",
    "        return print(jerk)\n",
    "\n",
    "    print('Would you like to SMOTE?\\nType \"1\" for Yes or \"2\" for No.\\n')\n",
    "    smt = input()\n",
    "    picks.append(int(smt))\n",
    "    if smt not in ['1','2']:\n",
    "        return print(jerk)\n",
    "\n",
    "    # Lemmatize function runs or not. The function requires the data to be in list form. The function will remove the list form.\n",
    "    # If not lemmatized, data['tokens'] column is taken out of list form.\n",
    "    if lemm == '1':\n",
    "        col = 'lemm'\n",
    "        lemmatize()\n",
    "        print('Lemmatized changed this many rows:', (len(data) - (sum(data['tokens'] == data['lemm']))),'\\n')\n",
    "    elif lemm == '2':\n",
    "        col = 'tokens'\n",
    "        data['tokens'] = data['tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "    # Here we compare the user's choices to our previously established lists. They are organized and hand picked from personal results to produce the best results.\n",
    "    if picks in log:\n",
    "        t = 0\n",
    "    elif picks in forest:\n",
    "        t = 1\n",
    "    elif picks in NB:\n",
    "        t = 2\n",
    "\n",
    "    # Train Test Split. Our column choice is based on lemmatization choice.\n",
    "    X_train, X_test, y_train, y_test = TTS(col)\n",
    "\n",
    "    # Count Vectorize or TF-IDF.\n",
    "    if model == '1':\n",
    "        X_train_counts, X_test_counts = CV(X_train, X_test)\n",
    "    elif model == '2':\n",
    "        X_test_counts, X_train_counts = tf_idf(X_train, X_test)\n",
    "\n",
    "    # SMOTE or not.\n",
    "    if smt == '1':\n",
    "        X_train_counts, y_train = smote(X_train_counts, y_train)\n",
    " \n",
    "    # Now that choices are made and we've picked our recommendation, we give the user a choice. They have control of the entire modeling process.\n",
    "    print('Would you like to pick your model or use our recomendation?\\nType \"1\" to choose or \"2\" to let us.')\n",
    "    reco = input()\n",
    "    if reco not in ['1','2']:\n",
    "        return print(jerk)\n",
    "    elif reco == '1':\n",
    "        print('\\n\\n\"1\" for Logistic Regression\\n\"2\" for Random Forest\\n\"3\" for Multinomial Naive Bayes')\n",
    "        t = (int(input()) - 1)\n",
    "\n",
    "    # t variable is either recommended by us or chosen by user. It picks an option from a list of models.\n",
    "    mod = choices[t]\n",
    "    if mod == logreg:\n",
    "        print('\\n\\nLogistic Regression Report')\n",
    "    elif mod == rf:\n",
    "        print('\\n\\nRandom Forest Report')\n",
    "    elif mod == multiNB:\n",
    "        print('\\n\\nMultinomial Naive Bayes')\n",
    "\n",
    "    # We run the chosen model and print out metrics for evaluation, along with a confusion matrix.\n",
    "    y_predicted_counts = mod(X_train_counts, y_train, X_test_counts)\n",
    "    classify(y_test, y_predicted_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "How would you like to analyze the data?\n",
      "Type \"1\" to Count Vectorize or \"2\" to implement TF-IDF.\n",
      "\n",
      "Would you like to lemmatize?\n",
      "Type \"1\" for Yes or \"2\" for No.\n",
      "\n",
      "Would you like to SMOTE?\n",
      "Type \"1\" for Yes or \"2\" for No.\n",
      "\n",
      "Lemmatized changed this many rows: 3539 \n",
      "\n",
      "Would you like to pick your model or use our recomendation?\n",
      "Type \"1\" to choose or \"2\" to let us.\n",
      "\n",
      "\n",
      "\"1\" for Logistic Regression\n",
      "\"2\" for Random Forest\n",
      "\"3\" for Multinomial Naive Bayes\n",
      "\n",
      "\n",
      "Logistic Regression Report\n",
      "\n",
      "\n",
      "Classification Report - TEST\n",
      "--------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.59      0.51       148\n",
      "           1       0.91      0.86      0.88       737\n",
      "\n",
      "    accuracy                           0.81       885\n",
      "   macro avg       0.68      0.72      0.70       885\n",
      "weighted avg       0.83      0.81      0.82       885\n",
      "\n",
      "--------------------------------------------------------------------------\n",
      "Confusion Matrix - TEST\n",
      "--------------------------------------------------------------------------\n",
      "Predicted    0    1  All\n",
      "True                    \n",
      "0           87   61  148\n",
      "1          106  631  737\n",
      "All        193  692  885\n",
      "--------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "user_models(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                   text   brand  feelings  \\\n",
       "0     .@wesley83 I have a 3G iPhone. After 3 hrs twe...   Apple         0   \n",
       "1     @jessedee Know about @fludapp ? Awesome iPad/i...   Apple         1   \n",
       "2     @swonderlin Can not wait for #iPad 2 also. The...   Apple         1   \n",
       "3     @sxsw I hope this year's festival isn't as cra...   Apple         0   \n",
       "4     @sxtxstate great stuff on Fri #SXSW: Marissa M...  Google         1   \n",
       "...                                                 ...     ...       ...   \n",
       "9077  @mention your PR guy just convinced me to swit...   Apple         1   \n",
       "9079  &quot;papyrus...sort of like the ipad&quot; - ...   Apple         1   \n",
       "9080  Diller says Google TV &quot;might be run over ...  Google         0   \n",
       "9085  I've always used Camera+ for my iPhone b/c it ...   Apple         1   \n",
       "9088                      Ipad everywhere. #SXSW {link}   Apple         1   \n",
       "\n",
       "                                                 tokens  \\\n",
       "0     [wesley83, 3g, 3, hrs, tweeting, rise_austin, ...   \n",
       "1     [jessedee, know, fludapp, awesome, likely, app...   \n",
       "2                     [swonderlin, wait, 2, also, sale]   \n",
       "3                  [hope, year, festival, crashy, year]   \n",
       "4     [sxtxstate, great, stuff, fri, marissa, mayer,...   \n",
       "...                                                 ...   \n",
       "9077  [pr, guy, convinced, switch, back, great, cove...   \n",
       "9079          [papyrus, sort, like, nice, lol, lavelle]   \n",
       "9080  [diller, says, tv, might, run, playstation, xb...   \n",
       "9085  [always, used, camera, b, c, image, stabilizer...   \n",
       "9088                                       [everywhere]   \n",
       "\n",
       "                                                   lemm  \n",
       "0     wesley83 3g 3 hr tweeting rise_austin dead nee...  \n",
       "1     jessedee know fludapp awesome likely appreciat...  \n",
       "2                           swonderlin wait 2 also sale  \n",
       "3                        hope year festival crashy year  \n",
       "4     sxtxstate great stuff fri marissa mayer tim re...  \n",
       "...                                                 ...  \n",
       "9077  pr guy convinced switch back great coverage pr...  \n",
       "9079                 papyrus sort like nice lol lavelle  \n",
       "9080  diller say tv might run playstation xbox essen...  \n",
       "9085  always used camera b c image stabilizer mode s...  \n",
       "9088                                         everywhere  \n",
       "\n",
       "[3539 rows x 5 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>brand</th>\n      <th>feelings</th>\n      <th>tokens</th>\n      <th>lemm</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n      <td>Apple</td>\n      <td>0</td>\n      <td>[wesley83, 3g, 3, hrs, tweeting, rise_austin, ...</td>\n      <td>wesley83 3g 3 hr tweeting rise_austin dead nee...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n      <td>Apple</td>\n      <td>1</td>\n      <td>[jessedee, know, fludapp, awesome, likely, app...</td>\n      <td>jessedee know fludapp awesome likely appreciat...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n      <td>Apple</td>\n      <td>1</td>\n      <td>[swonderlin, wait, 2, also, sale]</td>\n      <td>swonderlin wait 2 also sale</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>@sxsw I hope this year's festival isn't as cra...</td>\n      <td>Apple</td>\n      <td>0</td>\n      <td>[hope, year, festival, crashy, year]</td>\n      <td>hope year festival crashy year</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n      <td>Google</td>\n      <td>1</td>\n      <td>[sxtxstate, great, stuff, fri, marissa, mayer,...</td>\n      <td>sxtxstate great stuff fri marissa mayer tim re...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9077</th>\n      <td>@mention your PR guy just convinced me to swit...</td>\n      <td>Apple</td>\n      <td>1</td>\n      <td>[pr, guy, convinced, switch, back, great, cove...</td>\n      <td>pr guy convinced switch back great coverage pr...</td>\n    </tr>\n    <tr>\n      <th>9079</th>\n      <td>&amp;quot;papyrus...sort of like the ipad&amp;quot; - ...</td>\n      <td>Apple</td>\n      <td>1</td>\n      <td>[papyrus, sort, like, nice, lol, lavelle]</td>\n      <td>papyrus sort like nice lol lavelle</td>\n    </tr>\n    <tr>\n      <th>9080</th>\n      <td>Diller says Google TV &amp;quot;might be run over ...</td>\n      <td>Google</td>\n      <td>0</td>\n      <td>[diller, says, tv, might, run, playstation, xb...</td>\n      <td>diller say tv might run playstation xbox essen...</td>\n    </tr>\n    <tr>\n      <th>9085</th>\n      <td>I've always used Camera+ for my iPhone b/c it ...</td>\n      <td>Apple</td>\n      <td>1</td>\n      <td>[always, used, camera, b, c, image, stabilizer...</td>\n      <td>always used camera b c image stabilizer mode s...</td>\n    </tr>\n    <tr>\n      <th>9088</th>\n      <td>Ipad everywhere. #SXSW {link}</td>\n      <td>Apple</td>\n      <td>1</td>\n      <td>[everywhere]</td>\n      <td>everywhere</td>\n    </tr>\n  </tbody>\n</table>\n<p>3539 rows × 5 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "source": [
    "# Our Favorite Models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### We took an extremely scientific approach to analyzing our models: Predict, Experiment, Observe, Record the Results. With 3 possible models, a choice of processing the data, a choice to lemmatize and a choice to SMOTE, we end up with 24 (3 x 2 x 2 x 2) unique models. Observing the dataset to be imbalanced in favor of positive sentiments, we decided to focus primarily on the recall of the minority class. In practical terms, we are attempting to identify as many unhappy users of Google and Apple products as we can and prefer to err on the side of classifying extra unhappy users as oppoosed to missing them. \n",
    "\n",
    "### We run all 24 models individually and record their most important metrics: Accuracy, Precision, Recall, and F1 Score. Looking at the spreadsheet of results, a few things jump out at us. Excepting several outliers of lower recall, most of our recall scores are in the 0.40 - 0.60 range. Thus, we decided to factor in an overall metric, Accuracy when making our decisions. In each combination we use these metrics to choose our best model. In many categories it was a close decision and could have easily been justified to go in another direction."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "![spreadsheet](spreadsheetTop.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Our favorite model overall used Term Frequency–Inverse Document Frequency to transform our data, Lemmatization, Smoting, and used Multinomial Naive Bayes to model it. We used all the bells and whistles for our best result. It produced our third highest recall score, and a higher overall accuracy than the two models with higher recall. Again, it was a tough call and other model choices could have been justified just as well."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "![spreadsheet](spreadBottom.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### As highlighted above, recall was high in this model. The confusion matrix shows 86 correctly identified unhappy users, 51 unhappy users missed, and 126 happy users identified as unhappy. 51 missed users is tied with one other model as the lowest result we found. 126 happy users misidentified as unhappy is high, higher than we would like, but with over 600 correctly identified and a low number of unhappy users missed, we have hit our project target. The goal was to find unhappy users and overall accuracy and we are willing to accept a higher number of incorrectly identified users as long as they are happy. In this scenario, we would use these results to make unhappy users' experience better and the happy users may become even more positive."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}