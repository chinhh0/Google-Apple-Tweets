{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just a mess of imports. Much more to come.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk import FreqDist, word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, plot_confusion_matrix, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added the encoding because I was getting an error. Don't know about you.\n",
    "\n",
    "data = pd.read_csv('data.csv', encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 9093 entries, 0 to 9092\nData columns (total 3 columns):\n #   Column                                              Non-Null Count  Dtype \n---  ------                                              --------------  ----- \n 0   tweet_text                                          9092 non-null   object\n 1   emotion_in_tweet_is_directed_at                     3291 non-null   object\n 2   is_there_an_emotion_directed_at_a_brand_or_product  9093 non-null   object\ndtypes: object(3)\nmemory usage: 213.2+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "No emotion toward brand or product    5389\n",
       "Positive emotion                      2978\n",
       "Negative emotion                       570\n",
       "I can't tell                           156\n",
       "Name: is_there_an_emotion_directed_at_a_brand_or_product, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "data['is_there_an_emotion_directed_at_a_brand_or_product'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iPhone   \n",
       "1              iPad or iPhone App   \n",
       "2                            iPad   \n",
       "3              iPad or iPhone App   \n",
       "4                          Google   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                   Negative emotion  \n",
       "1                                   Positive emotion  \n",
       "2                                   Positive emotion  \n",
       "3                                   Negative emotion  \n",
       "4                                   Positive emotion  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet_text</th>\n      <th>emotion_in_tweet_is_directed_at</th>\n      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n      <td>iPhone</td>\n      <td>Negative emotion</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n      <td>iPad or iPhone App</td>\n      <td>Positive emotion</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n      <td>iPad</td>\n      <td>Positive emotion</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>@sxsw I hope this year's festival isn't as cra...</td>\n      <td>iPad or iPhone App</td>\n      <td>Negative emotion</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n      <td>Google</td>\n      <td>Positive emotion</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'.@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.'"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "data.iloc[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                             tweet_text  \\\n",
       "5     @teachntech00 New iPad Apps For #SpeechTherapy...   \n",
       "6                                                   NaN   \n",
       "16    Holler Gram for iPad on the iTunes App Store -...   \n",
       "32    Attn: All  #SXSW frineds, @mention Register fo...   \n",
       "33        Anyone at  #sxsw want to sell their old iPad?   \n",
       "...                                                 ...   \n",
       "9087  @mention Yup, but I don't have a third app yet...   \n",
       "9089  Wave, buzz... RT @mention We interrupt your re...   \n",
       "9090  Google's Zeiger, a physician never reported po...   \n",
       "9091  Some Verizon iPhone customers complained their...   \n",
       "9092  Ï¡Ïàü_ÊÎÒ£Áââ_£â_ÛâRT @...   \n",
       "\n",
       "     emotion_in_tweet_is_directed_at  \\\n",
       "5                                NaN   \n",
       "6                                NaN   \n",
       "16                               NaN   \n",
       "32                               NaN   \n",
       "33                               NaN   \n",
       "...                              ...   \n",
       "9087                             NaN   \n",
       "9089                             NaN   \n",
       "9090                             NaN   \n",
       "9091                             NaN   \n",
       "9092                             NaN   \n",
       "\n",
       "     is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "5                    No emotion toward brand or product  \n",
       "6                    No emotion toward brand or product  \n",
       "16                   No emotion toward brand or product  \n",
       "32                   No emotion toward brand or product  \n",
       "33                   No emotion toward brand or product  \n",
       "...                                                 ...  \n",
       "9087                 No emotion toward brand or product  \n",
       "9089                 No emotion toward brand or product  \n",
       "9090                 No emotion toward brand or product  \n",
       "9091                 No emotion toward brand or product  \n",
       "9092                 No emotion toward brand or product  \n",
       "\n",
       "[5802 rows x 3 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet_text</th>\n      <th>emotion_in_tweet_is_directed_at</th>\n      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>5</th>\n      <td>@teachntech00 New iPad Apps For #SpeechTherapy...</td>\n      <td>NaN</td>\n      <td>No emotion toward brand or product</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>No emotion toward brand or product</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>Holler Gram for iPad on the iTunes App Store -...</td>\n      <td>NaN</td>\n      <td>No emotion toward brand or product</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>Attn: All  #SXSW frineds, @mention Register fo...</td>\n      <td>NaN</td>\n      <td>No emotion toward brand or product</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>Anyone at  #sxsw want to sell their old iPad?</td>\n      <td>NaN</td>\n      <td>No emotion toward brand or product</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9087</th>\n      <td>@mention Yup, but I don't have a third app yet...</td>\n      <td>NaN</td>\n      <td>No emotion toward brand or product</td>\n    </tr>\n    <tr>\n      <th>9089</th>\n      <td>Wave, buzz... RT @mention We interrupt your re...</td>\n      <td>NaN</td>\n      <td>No emotion toward brand or product</td>\n    </tr>\n    <tr>\n      <th>9090</th>\n      <td>Google's Zeiger, a physician never reported po...</td>\n      <td>NaN</td>\n      <td>No emotion toward brand or product</td>\n    </tr>\n    <tr>\n      <th>9091</th>\n      <td>Some Verizon iPhone customers complained their...</td>\n      <td>NaN</td>\n      <td>No emotion toward brand or product</td>\n    </tr>\n    <tr>\n      <th>9092</th>\n      <td>Ï¡Ïàü_ÊÎÒ£Áââ_£â_ÛâRT @...</td>\n      <td>NaN</td>\n      <td>No emotion toward brand or product</td>\n    </tr>\n  </tbody>\n</table>\n<p>5802 rows × 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "data[data.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "iPad                               946\n",
       "Apple                              661\n",
       "iPad or iPhone App                 470\n",
       "Google                             430\n",
       "iPhone                             297\n",
       "Other Google product or service    293\n",
       "Android App                         81\n",
       "Android                             78\n",
       "Other Apple product or service      35\n",
       "Name: emotion_in_tweet_is_directed_at, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "data['emotion_in_tweet_is_directed_at'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplify\n",
    "\n",
    "company = {'iPad': 'Apple',\n",
    "            'Apple': 'Apple',\n",
    "            'iPad or iPhone App': 'Apple',\n",
    "            'Google': 'Google',\n",
    "            'iPhone': 'Apple',\n",
    "            'Other Google product or service': 'Google',\n",
    "            'Android App': 'Google',\n",
    "            'Android': 'Google',\n",
    "            'Other Apple product or service': 'Apple'}\n",
    "data['emotion_in_tweet_is_directed_at'] = data['emotion_in_tweet_is_directed_at'].map(company)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "No emotion toward brand or product    5389\n",
       "Positive emotion                      2978\n",
       "Negative emotion                       570\n",
       "I can't tell                           156\n",
       "Name: is_there_an_emotion_directed_at_a_brand_or_product, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "data['is_there_an_emotion_directed_at_a_brand_or_product'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(data['tweet_text'].isna().sum())\n",
    "\n",
    "data = data[~data['tweet_text'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplify column names\n",
    "\n",
    "data.rename(columns={'tweet_text': 'text', 'emotion_in_tweet_is_directed_at': 'brand', 'is_there_an_emotion_directed_at_a_brand_or_product': 'feelings'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### Simplify for now to create base model. Afterwards, may revisit. #####################\n",
    "\n",
    "data.drop(data[data['feelings'] == \"I can't tell\"].index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "\n",
    "feels = {'Negative emotion': 0,\n",
    "        'Positive emotion': 1,\n",
    "        'No emotion toward brand or product': 2}\n",
    "\n",
    "data['feelings'] = data['feelings'].map(feels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "8936"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### Simplify for now to create base model. Afterwards, definitely revisit. #####################\n",
    "\n",
    "data = data[data['feelings'] <= 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3548"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['text'].map(word_tokenize)\n",
    "target = data['feelings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "hashtag = RegexpTokenizer(r'\\#\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                   text   brand  feelings\n",
       "0     .@wesley83 I have a 3G iPhone. After 3 hrs twe...   Apple         0\n",
       "1     @jessedee Know about @fludapp ? Awesome iPad/i...   Apple         1\n",
       "2     @swonderlin Can not wait for #iPad 2 also. The...   Apple         1\n",
       "3     @sxsw I hope this year's festival isn't as cra...   Apple         0\n",
       "4     @sxtxstate great stuff on Fri #SXSW: Marissa M...  Google         1\n",
       "...                                                 ...     ...       ...\n",
       "9077  @mention your PR guy just convinced me to swit...   Apple         1\n",
       "9079  &quot;papyrus...sort of like the ipad&quot; - ...   Apple         1\n",
       "9080  Diller says Google TV &quot;might be run over ...  Google         0\n",
       "9085  I've always used Camera+ for my iPhone b/c it ...   Apple         1\n",
       "9088                      Ipad everywhere. #SXSW {link}   Apple         1\n",
       "\n",
       "[3548 rows x 3 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>brand</th>\n      <th>feelings</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n      <td>Apple</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n      <td>Apple</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n      <td>Apple</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>@sxsw I hope this year's festival isn't as cra...</td>\n      <td>Apple</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n      <td>Google</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9077</th>\n      <td>@mention your PR guy just convinced me to swit...</td>\n      <td>Apple</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9079</th>\n      <td>&amp;quot;papyrus...sort of like the ipad&amp;quot; - ...</td>\n      <td>Apple</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9080</th>\n      <td>Diller says Google TV &amp;quot;might be run over ...</td>\n      <td>Google</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9085</th>\n      <td>I've always used Camera+ for my iPhone b/c it ...</td>\n      <td>Apple</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9088</th>\n      <td>Ipad everywhere. #SXSW {link}</td>\n      <td>Apple</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>3548 rows × 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = stopwords.words('english')\n",
    "stops += list(string.punctuation)\n",
    "stops.extend(['sxsw', 'sxswi', 'quot', 'mention', 'link', 'rt', 'amp', 'http', 'sxswrt', 'google', 'googles', 'app', 'apps', 'android', 'austin', 'quotgoogle', 'new', 'today', 'one', 'apple', 'ipad', 'iphone', 'ipad2', 'apples', 'quotapple'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                   text   brand  feelings  \\\n",
       "0     .@wesley83 I have a 3G iPhone. After 3 hrs twe...   Apple         0   \n",
       "1     @jessedee Know about @fludapp ? Awesome iPad/i...   Apple         1   \n",
       "2     @swonderlin Can not wait for #iPad 2 also. The...   Apple         1   \n",
       "3     @sxsw I hope this year's festival isn't as cra...   Apple         0   \n",
       "4     @sxtxstate great stuff on Fri #SXSW: Marissa M...  Google         1   \n",
       "...                                                 ...     ...       ...   \n",
       "9077  @mention your PR guy just convinced me to swit...   Apple         1   \n",
       "9079  &quot;papyrus...sort of like the ipad&quot; - ...   Apple         1   \n",
       "9080  Diller says Google TV &quot;might be run over ...  Google         0   \n",
       "9085  I've always used Camera+ for my iPhone b/c it ...   Apple         1   \n",
       "9088                      Ipad everywhere. #SXSW {link}   Apple         1   \n",
       "\n",
       "                                                 tokens                   hash  \n",
       "0     [wesley83, I, have, a, 3G, iPhone, After, 3, h...  [#RISE_Austin, #SXSW]  \n",
       "1     [jessedee, Know, about, fludapp, Awesome, iPad...                [#SXSW]  \n",
       "2     [swonderlin, Can, not, wait, for, iPad, 2, als...         [#iPad, #SXSW]  \n",
       "3     [sxsw, I, hope, this, year, s, festival, isn, ...                [#sxsw]  \n",
       "4     [sxtxstate, great, stuff, on, Fri, SXSW, Maris...                [#SXSW]  \n",
       "...                                                 ...                    ...  \n",
       "9077  [mention, your, PR, guy, just, convinced, me, ...     [#sxsw, #princess]  \n",
       "9079  [quot, papyrus, sort, of, like, the, ipad, quo...                [#SXSW]  \n",
       "9080  [Diller, says, Google, TV, quot, might, be, ru...       [#sxsw, #diller]  \n",
       "9085  [I, ve, always, used, Camera, for, my, iPhone,...        [#SXSW, #SXSWi]  \n",
       "9088                     [Ipad, everywhere, SXSW, link]                [#SXSW]  \n",
       "\n",
       "[3548 rows x 5 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>brand</th>\n      <th>feelings</th>\n      <th>tokens</th>\n      <th>hash</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n      <td>Apple</td>\n      <td>0</td>\n      <td>[wesley83, I, have, a, 3G, iPhone, After, 3, h...</td>\n      <td>[#RISE_Austin, #SXSW]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n      <td>Apple</td>\n      <td>1</td>\n      <td>[jessedee, Know, about, fludapp, Awesome, iPad...</td>\n      <td>[#SXSW]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n      <td>Apple</td>\n      <td>1</td>\n      <td>[swonderlin, Can, not, wait, for, iPad, 2, als...</td>\n      <td>[#iPad, #SXSW]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>@sxsw I hope this year's festival isn't as cra...</td>\n      <td>Apple</td>\n      <td>0</td>\n      <td>[sxsw, I, hope, this, year, s, festival, isn, ...</td>\n      <td>[#sxsw]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n      <td>Google</td>\n      <td>1</td>\n      <td>[sxtxstate, great, stuff, on, Fri, SXSW, Maris...</td>\n      <td>[#SXSW]</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9077</th>\n      <td>@mention your PR guy just convinced me to swit...</td>\n      <td>Apple</td>\n      <td>1</td>\n      <td>[mention, your, PR, guy, just, convinced, me, ...</td>\n      <td>[#sxsw, #princess]</td>\n    </tr>\n    <tr>\n      <th>9079</th>\n      <td>&amp;quot;papyrus...sort of like the ipad&amp;quot; - ...</td>\n      <td>Apple</td>\n      <td>1</td>\n      <td>[quot, papyrus, sort, of, like, the, ipad, quo...</td>\n      <td>[#SXSW]</td>\n    </tr>\n    <tr>\n      <th>9080</th>\n      <td>Diller says Google TV &amp;quot;might be run over ...</td>\n      <td>Google</td>\n      <td>0</td>\n      <td>[Diller, says, Google, TV, quot, might, be, ru...</td>\n      <td>[#sxsw, #diller]</td>\n    </tr>\n    <tr>\n      <th>9085</th>\n      <td>I've always used Camera+ for my iPhone b/c it ...</td>\n      <td>Apple</td>\n      <td>1</td>\n      <td>[I, ve, always, used, Camera, for, my, iPhone,...</td>\n      <td>[#SXSW, #SXSWi]</td>\n    </tr>\n    <tr>\n      <th>9088</th>\n      <td>Ipad everywhere. #SXSW {link}</td>\n      <td>Apple</td>\n      <td>1</td>\n      <td>[Ipad, everywhere, SXSW, link]</td>\n      <td>[#SXSW]</td>\n    </tr>\n  </tbody>\n</table>\n<p>3548 rows × 5 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "data['tokens'] = data['text'].apply(tokenizer.tokenize)\n",
    "data['hash'] = data['text'].apply(hashtag.tokenize)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokens'] = data['tokens'].apply(lambda x: [word.lower() for word in x if word not in stops])\n",
    "data['tokens'] = data['tokens'].apply(lambda x: [word.lower() for word in x if word not in stops])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                   text   brand  feelings  \\\n",
       "0     .@wesley83 I have a 3G iPhone. After 3 hrs twe...   Apple         0   \n",
       "1     @jessedee Know about @fludapp ? Awesome iPad/i...   Apple         1   \n",
       "2     @swonderlin Can not wait for #iPad 2 also. The...   Apple         1   \n",
       "3     @sxsw I hope this year's festival isn't as cra...   Apple         0   \n",
       "4     @sxtxstate great stuff on Fri #SXSW: Marissa M...  Google         1   \n",
       "...                                                 ...     ...       ...   \n",
       "9077  @mention your PR guy just convinced me to swit...   Apple         1   \n",
       "9079  &quot;papyrus...sort of like the ipad&quot; - ...   Apple         1   \n",
       "9080  Diller says Google TV &quot;might be run over ...  Google         0   \n",
       "9085  I've always used Camera+ for my iPhone b/c it ...   Apple         1   \n",
       "9088                      Ipad everywhere. #SXSW {link}   Apple         1   \n",
       "\n",
       "                                                 tokens                   hash  \n",
       "0     [wesley83, i, 3g, iphone, after, 3, hrs, tweet...  [#RISE_Austin, #SXSW]  \n",
       "1     [jessedee, know, fludapp, awesome, ipad, iphon...                [#SXSW]  \n",
       "2     [swonderlin, can, wait, ipad, 2, also, they, s...         [#iPad, #SXSW]  \n",
       "3       [i, hope, year, festival, crashy, year, iphone]                [#sxsw]  \n",
       "4     [sxtxstate, great, stuff, fri, sxsw, marissa, ...                [#SXSW]  \n",
       "...                                                 ...                    ...  \n",
       "9077  [pr, guy, convinced, switch, back, iphone, gre...     [#sxsw, #princess]  \n",
       "9079    [papyrus, sort, like, nice, lol, sxsw, lavelle]                [#SXSW]  \n",
       "9080  [diller, says, google, tv, might, run, playsta...       [#sxsw, #diller]  \n",
       "9085  [i, always, used, camera, iphone, b, c, image,...        [#SXSW, #SXSWi]  \n",
       "9088                           [ipad, everywhere, sxsw]                [#SXSW]  \n",
       "\n",
       "[3548 rows x 5 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>brand</th>\n      <th>feelings</th>\n      <th>tokens</th>\n      <th>hash</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n      <td>Apple</td>\n      <td>0</td>\n      <td>[wesley83, i, 3g, iphone, after, 3, hrs, tweet...</td>\n      <td>[#RISE_Austin, #SXSW]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n      <td>Apple</td>\n      <td>1</td>\n      <td>[jessedee, know, fludapp, awesome, ipad, iphon...</td>\n      <td>[#SXSW]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n      <td>Apple</td>\n      <td>1</td>\n      <td>[swonderlin, can, wait, ipad, 2, also, they, s...</td>\n      <td>[#iPad, #SXSW]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>@sxsw I hope this year's festival isn't as cra...</td>\n      <td>Apple</td>\n      <td>0</td>\n      <td>[i, hope, year, festival, crashy, year, iphone]</td>\n      <td>[#sxsw]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n      <td>Google</td>\n      <td>1</td>\n      <td>[sxtxstate, great, stuff, fri, sxsw, marissa, ...</td>\n      <td>[#SXSW]</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9077</th>\n      <td>@mention your PR guy just convinced me to swit...</td>\n      <td>Apple</td>\n      <td>1</td>\n      <td>[pr, guy, convinced, switch, back, iphone, gre...</td>\n      <td>[#sxsw, #princess]</td>\n    </tr>\n    <tr>\n      <th>9079</th>\n      <td>&amp;quot;papyrus...sort of like the ipad&amp;quot; - ...</td>\n      <td>Apple</td>\n      <td>1</td>\n      <td>[papyrus, sort, like, nice, lol, sxsw, lavelle]</td>\n      <td>[#SXSW]</td>\n    </tr>\n    <tr>\n      <th>9080</th>\n      <td>Diller says Google TV &amp;quot;might be run over ...</td>\n      <td>Google</td>\n      <td>0</td>\n      <td>[diller, says, google, tv, might, run, playsta...</td>\n      <td>[#sxsw, #diller]</td>\n    </tr>\n    <tr>\n      <th>9085</th>\n      <td>I've always used Camera+ for my iPhone b/c it ...</td>\n      <td>Apple</td>\n      <td>1</td>\n      <td>[i, always, used, camera, iphone, b, c, image,...</td>\n      <td>[#SXSW, #SXSWi]</td>\n    </tr>\n    <tr>\n      <th>9088</th>\n      <td>Ipad everywhere. #SXSW {link}</td>\n      <td>Apple</td>\n      <td>1</td>\n      <td>[ipad, everywhere, sxsw]</td>\n      <td>[#SXSW]</td>\n    </tr>\n  </tbody>\n</table>\n<p>3548 rows × 5 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ CHANGE BELOW TO TF IDF ####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Counter({1: 2249, 0: 2249})\n"
     ]
    }
   ],
   "source": [
    "smote = SMOTE()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['feelings'])\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "X_train_counts = count_vectorizer.fit_transform(X_train)\n",
    "X_test_counts = count_vectorizer.transform(X_test)\n",
    "\n",
    "X_train_counts, y_train = smote.fit_sample(X_train_counts, y_train)\n",
    "\n",
    "print(Counter(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Straight up stolen code, tweak to needs later or discard because it's useless\n",
    "# Haven't bothered changing labels because the graph isn't useful for our purposes\n",
    "\n",
    "# from sklearn.decomposition import PCA, TruncatedSVD\n",
    "# import matplotlib\n",
    "# import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "# def plot_LSA(test_data, test_labels, savepath=\"PCA_demo.csv\", plot=True):\n",
    "#         lsa = TruncatedSVD(n_components=2)\n",
    "#         lsa.fit(test_data)\n",
    "#         lsa_scores = lsa.transform(test_data)\n",
    "#         color_mapper = {label:idx for idx,label in enumerate(set(test_labels))}\n",
    "#         color_column = [color_mapper[label] for label in test_labels]\n",
    "#         colors = ['orange','blue','blue']\n",
    "#         if plot:\n",
    "#             plt.scatter(lsa_scores[:,0], lsa_scores[:,1], s=8, alpha=.8, c=test_labels, cmap=matplotlib.colors.ListedColormap(colors))\n",
    "#             red_patch = mpatches.Patch(color='orange', label='Irrelevant')\n",
    "#             green_patch = mpatches.Patch(color='blue', label='Disaster')\n",
    "#             plt.legend(handles=[red_patch, green_patch], prop={'size': 30})\n",
    "\n",
    "\n",
    "# fig = plt.figure(figsize=(16, 16))          \n",
    "# plot_LSA(X_train_counts, y_train)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "clf = LogisticRegression(C=30.0, class_weight='balanced', solver='newton-cg', \n",
    "                         multi_class='multinomial', n_jobs=-1, random_state=40)\n",
    "clf.fit(X_train_counts, y_train)\n",
    "\n",
    "y_predicted_counts = clf.predict(X_test_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "accuracy = 0.818, precision = 0.830, recall = 0.818, f1 = 0.824\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def get_metrics(y_test, y_predicted):  \n",
    "    # true positives / (true positives+false positives)\n",
    "    precision = precision_score(y_test, y_predicted, pos_label=None,\n",
    "                                    average='weighted')             \n",
    "    # true positives / (true positives + false negatives)\n",
    "    recall = recall_score(y_test, y_predicted, pos_label=None,\n",
    "                              average='weighted')\n",
    "    \n",
    "    # harmonic mean of precision and recall\n",
    "    f1 = f1_score(y_test, y_predicted, pos_label=None, average='weighted')\n",
    "    \n",
    "    # true positives + true negatives/ total\n",
    "    accuracy = accuracy_score(y_test, y_predicted)\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "accuracy, precision, recall, f1 = get_metrics(y_test, y_predicted_counts)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ 90,  68],\n",
       "       [ 93, 636]], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_predicted_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "accuracy = 0.789, precision = 0.804, recall = 0.789, f1 = 0.796\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train_counts, y_train)\n",
    "\n",
    "y_predicted_counts = rf.predict(X_test_counts)\n",
    "\n",
    "accuracy, precision, recall, f1 = get_metrics(y_test, y_predicted_counts)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ 79,  79],\n",
       "       [108, 621]], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_predicted_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'auto',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': None,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "rf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 3 folds for each of 576 candidates, totalling 1728 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    9.1s\n",
      "[Parallel(n_jobs=-1)]: Done 333 tasks      | elapsed:   19.8s\n",
      "[Parallel(n_jobs=-1)]: Done 616 tasks      | elapsed:   33.5s\n",
      "[Parallel(n_jobs=-1)]: Done 981 tasks      | elapsed:   59.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1426 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1728 out of 1728 | elapsed:  1.9min finished\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'bootstrap': [True, False],\n",
    "    'max_depth': [None, 70, 80, 90],\n",
    "    'max_features': ['auto'],\n",
    "    'min_samples_leaf': [1, 2, 3, 4],\n",
    "    'min_samples_split': [2, 3, 4, 5, 6, 7],\n",
    "    'n_estimators': [65, 70, 75]\n",
    "}\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, scoring='f1', \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)\n",
    "\n",
    "grid_search.fit(X_train_counts, y_train)\n",
    "\n",
    "y_predicted_counts = grid_search.predict(X_test_counts)\n",
    "\n",
    "accuracy, precision, recall, f1 = get_metrics(y_test, y_predicted_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'bootstrap': False,\n",
       " 'max_depth': 80,\n",
       " 'max_features': 'auto',\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 6,\n",
       " 'n_estimators': 65}"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9028972489136301"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "accuracy = 0.832, precision = 0.818, recall = 0.832, f1 = 0.824\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ 66,  92],\n",
       "       [ 57, 672]], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_predicted_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "accuracy = 0.808, precision = 0.810, recall = 0.808, f1 = 0.809\n"
     ]
    }
   ],
   "source": [
    "nb.fit(X_train_counts, y_train)\n",
    "\n",
    "y_predicted_counts = nb.predict(X_test_counts)\n",
    "\n",
    "accuracy, precision, recall, f1 = get_metrics(y_test, y_predicted_counts)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ 75,  83],\n",
       "       [ 87, 642]], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_predicted_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. tf-idf\n",
    "# 2. SMOTE\n",
    "# 3. Lemmatize\n",
    "# 4. Bigrams + Trigrams"
   ]
  }
 ]
}